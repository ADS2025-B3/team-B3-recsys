# Model Lifecycle Stages

## ğŸ“‹ Overview

The **Model Lifecycle** describes the complete journey of a machine learning model from conception to deployment and maintenance. This document outlines the key stages, responsibilities, and deliverables at each phase for the Recommender System project.

---

## ğŸ”„ Lifecycle Stages

### 1. **Problem Definition & Planning**

**Objective:** Clearly articulate the business problem, constraints, and success criteria.

**Key Activities:**
- Define recommendation objectives (e.g., movie recommendations based on user preferences)
- Establish performance metrics (RMSE, MAE, Precision@K, Recall@K, NDCG)
- Identify data requirements and availability
- Set project timeline and resource allocation
- Document assumptions and constraints

**Deliverables:**
- Product Backlog with Epics and User Stories
- Success metrics and acceptance criteria
- Data availability assessment

**Tools & Artifacts:**
- GitHub Issues (Epics, User Stories)
- Excel Sprint Backlog (`ph_product_sprint_backlog.xlsm`)
- Metrics documentation in notebooks

**Team Roles:**
- Product Owner: Defines requirements
- Data Scientists: Assess feasibility

---

### 2. **Data Preparation & Exploration**

**Objective:** Collect, clean, and understand the data that will train and evaluate models.

**Key Activities:**
- Collect raw data (MovieLens dataset, user interactions)
- Perform exploratory data analysis (EDA)
- Handle missing values, duplicates, and outliers
- Feature engineering (user profiles, movie metadata)
- Data versioning with DVC (Data Version Control)
- Train/test/validation split

**Deliverables:**
- Clean, processed datasets (`data/processed/train.csv`, `data/processed/test.csv`)
- EDA reports and visualizations
- Data pipeline documentation
- DVC tracking files (`data/raw/*.dvc`)

**Tools & Artifacts:**
- Jupyter notebooks: `/notebooks/*.ipynb`
- DVC pipeline configuration
- `data/` directory structure
- Scripts: `/scripts/load_movies.py`, `/scripts/preprocess_movies.py`

**Team Roles:**
- DevOps Engineer: Pipeline development
- Data Scientist: EDA and feature engineering

---

### 3. **Model Development & Experimentation**

**Objective:** Build, train, and compare multiple model architectures to identify the best candidate.

**Key Activities:**
- Implement baseline models (collaborative filtering, content-based)
- Experiment with advanced models (SVD, neural networks)
- Hyperparameter tuning (grid search, random search)
- Cross-validation and performance evaluation
- Track experiments with MLflow
- Version model artifacts

**Deliverables:**
- Trained model files in `/models/`
- Experiment results tracked in MLflow
- Comparison reports
- Model hyperparameter configurations
- Code in `/src/` and `/back-end/app/`

**Tools & Artifacts:**
- MLflow Tracking Server (`mlflow_server/`)
- Experiment notebooks: `/notebooks/10MOptimization.ipynb`, `OptimizationModelParket.ipynb`
- Model implementations: `/back-end/svd_impl.py`, `/retraining-service/train_svd_user.py`
- Grid search results: `/outputs/grid_search_results.csv`
- Configuration: `/configs/params.yaml`

**Team Roles:**
- Data Scientist: Model development and experimentation
- DevOps Engineer: Experiment tracking and reproducibility

---

### 4. **Model Validation & Testing**

**Objective:** Rigorously evaluate model performance on held-out data and validate assumptions.

**Key Activities:**
- Evaluate on test set (unseen during training)
- Perform cross-validation
- Analyze model predictions for bias and fairness
- A/B testing considerations
- Sensitivity analysis
- Performance benchmarking against baselines

**Deliverables:**
- Validation reports with metrics
- Test results and performance benchmarks
- Model performance documentation

**Tools & Artifacts:**
- Test datasets: `data/processed/test.csv`
- Validation scripts
- MLflow evaluation metrics
- Performance comparison charts

**Team Roles:**
- Data Scientist: Analysis and interpretation

---

### 5. **Model Review & Approval**

**Objective:** Ensure the model meets business requirements and is ready for deployment.

**Key Activities:**
- Review model performance against success criteria
- Assess model interpretability and explainability
- Check for data drift and stability
- Security and privacy review
- Compliance and governance checks
- Stakeholder approval

**Deliverables:**
- Model review document
- Approval sign-off
- Risk assessment
- Deployment checklist

**Tools & Artifacts:**
- Review documentation
- Compliance checklists
- Sign-off records

**Team Roles:**
- Product Owner: Business approval
- Data Scientists: Technical review

---

### 6. **Model Deployment**

**Objective:** Move the validated model to production where it serves real recommendations.

**Key Activities:**
- Containerize the model (Docker)
- Set up inference API (`/back-end/app/api/`)
- Deploy to production infrastructure
- Configure monitoring and logging
- Set up fallback mechanisms
- Document deployment procedures

**Deliverables:**
- Docker images (`Dockerfile`, `Dockerfile.experiments`)
- Deployed API endpoints
- Deployment documentation
- Infrastructure configuration (`docker-compose.yml`)

**Tools & Artifacts:**
- Docker containers and registry
- API service code: `/back-end/app/main.py`, `/back-end/app/api/`
- Database setup: PostgreSQL (`postgres_data/`)
- Front-end interface: `/front-end/`
- Deployment logs: `/back-end/logs/`

**Team Roles:**
- DevOps Engineer: Deployment and infrastructure
- Backend Developer: API implementation

---

### 7. **Model Monitoring & Maintenance**

**Objective:** Continuously track model performance in production and maintain its effectiveness.

**Key Activities:**
- Monitor prediction accuracy and latency
- Detect data drift and concept drift
- Track model predictions and user feedback
- Identify when retraining is needed
- Implement automated alerting
- Maintain model documentation

**Deliverables:**
- Monitoring dashboards
- Drift reports
- Maintenance logs
- Performance metrics tracking

**Tools & Artifacts:**
- Monitoring scripts: `/retraining-service/`
- MLflow Tracking Server (Deployed): `mlflow_server/`
- Logging configuration: `/back-end/logs/`
- Dashboard configurations
- Alert thresholds and rules

**Team Roles:**
- DevOps Engineer: Monitoring setup and maintenance, MLflow server infrastructure and maintenance
- Data Engineer: Data pipeline health

---

### 8. **Model Retraining & Iteration**

**Objective:** Update the model with new data and improvements to maintain performance.

**Key Activities:**
- Collect new training data
- Retrain model on updated dataset
- Compare performance with current production model
- Re-run validation pipeline
- A/B test new model (if applicable)
- Deploy updated model or rollback if needed

**Deliverables:**
- Retrained model artifacts
- New experiment results
- Comparison reports
- Retraining documentation

**Tools & Artifacts:**
- Retraining service: `/retraining-service/`
- Scheduler: `/retraining-service/Scheduler.py`
- Updated datasets in `/data/`
- New experiment tracking in MLflow
- Model versioning in registry

**Team Roles:**
- Data Scientist: Retraining strategy
- DevOps: Automation, scheduling and data pipeline updates

---

### 9. **Model Retirement & Archival**

**Objective:** Systematically retire models that are no longer useful and maintain historical records.

**Key Activities:**
- Identify reasons for retirement (poor performance, obsolescence, business change)
- Plan migration path for dependent systems
- Archive model artifacts and documentation
- Update systems to use replacement model
- Document lessons learned

**Deliverables:**
- Retirement documentation
- Migration plan
- Archived model records
- Post-mortem analysis (if applicable)

**Tools & Artifacts:**
- Version control history
- Model registry archives
- Documentation in notebooks and markdown files

**Team Roles:**
- Product Owner: Business decision
- Data Scientist: Documentation and analysis

---

## ğŸ“Š Lifecycle Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MODEL LIFECYCLE FLOW                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    1. Problem           2. Data              3. Model           4. Validation
    Definition          Preparation          Development        & Testing
        â”‚                   â”‚                    â”‚                   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
                        5. Review & Approval
                                    â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚                       â”‚
                    APPROVED              REJECTED
                        â”‚                       â”‚
                        â–¼                       â–¼
                6. Deployment          Re-development
                        â”‚                    (Stage 3)
                        â–¼
        7. Monitoring & Maintenance
                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚               â”‚               â”‚
    HEALTHY      DATA DRIFT        PERFORMANCE
                                    DEGRADATION
        â”‚               â”‚               â”‚
        â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚                               â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
    8. Retraining & Iteration â”€â”€â”€â”€â”€â”€â”
        â”‚                            â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                                     â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
            Is Model Still Useful?
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                       â”‚
       YES                      NO
        â”‚                       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼ (if YES, loop back to Stage 7)
                      â”‚
                      â”‚ (if NO)
                      â–¼
            9. Retirement & Archival
```

---

## ğŸ”§ Key Tools & Systems by Stage

| Stage | Tools | System | Location |
|-------|-------|--------|----------|
| Planning | GitHub Issues, Excel | Backlog Management | `ph_product_sprint_backlog.xlsm` |
| Data Prep | Python, DVC, Pandas | Data Pipeline | `scripts/`, `data/` |
| Development | Jupyter, Python, scikit-learn | ML Experiments | `notebooks/`, `src/` |
| Validation | Python, Jupyter | Testing | `notebooks/`, Tests |
| Testing | MLflow, Python | Experiment Tracking | `mlflow_server/` |
| Deployment | Docker, Docker Compose | Container Services | `Dockerfile*`, `docker-compose.yml` |
| API Serving | FastAPI, Python | Backend Service | `back-end/app/` |
| Monitoring | MLflow (Deployed), Python | Retraining Service & Tracking | `mlflow_server/`, `retraining-service/` |
| Retraining | Python, Scheduler | Automation | `retraining-service/Scheduler.py` |

---

## ğŸ“Œ Success Criteria by Stage

| Stage | Success Criteria |
|-------|------------------|
| Problem Definition | Clear metrics defined, stakeholder alignment |
| Data Preparation | Complete datasets, no missing values, DVC tracked |
| Model Development | Baseline + advanced models compared, experiments tracked |
| Validation | Target metrics achieved, robust on test set |
| Review & Approval | Business & technical approval obtained |
| Deployment | API serving, monitoring active, zero-downtime deployment |
| Monitoring | Alerts configured, metrics trending positively |
| Retraining | Automated triggers working, new model validated |
| Retirement | Successor deployed, data archived, lessons documented |

---

## ğŸš€ Getting Started

To follow this lifecycle in your team:

1. **Stage 1-2:** Define problems and prepare data (Sprint 1)
2. **Stage 3-4:** Develop and validate models (Sprint 2)
3. **Stage 5-6:** Review and deploy to production (Sprint 3)
4. **Stage 7-9:** Monitor, retrain, and maintain (Ongoing)

Track progress in the **GitHub Project Boards** and **Excel tracker** as you advance through each stage.

---

## ğŸ“š Additional Resources

- **STUDENT_GUIDE.md** â€” Detailed team setup and process
- **MLFLOW Documentation** â€” Experiment tracking: `mlflow_server/readme.md`
- **Backend README** â€” API deployment: `back-end/readme.md`
- **Frontend README** â€” UI setup: `front-end/readme.md`

---

**Last Updated:** December 2025  
**Team:** B3 Recommender System  
**Project:** Agile Data Science - ML Model Lifecycle Management
